is transfer learning/pretrainng able to be applied here?

TODO
- LR scheduling - keep experimenting
- deal with weird noise issue in images, ruining model on some examples
- masked attention layer like pconv?
- implement pixel recursive sr
- ray tune
- more metrics?
- profiler

Models
- SRCNN - basic model, used to test dataset
- some other ConvNet for SR
- SRGAN?
- PConv - augment with attention?
- PixelSNAIL but tiny?
- Image Transformer but tiny

Tuning
- learning rate, learning schedule
- layer parameters
- conv first, attention alter
- residual connections

Metrics
- if probabilistic model, then NLL
- measure efficiency, theres probably a library for it
- MOS using Mechanical Turk???

Training setups
python train.py --model SRCNN --loss L2 --learning_rate 0.001 --lr_size 16 --up_factor 2 --epochs 1 --num_patches 4 --ckpt_name SRCNN
python inference.py --model SRCNN --loss L2 --lr_size 16 --up_factor 2 --ckpt_name SRCNN

python train.py --model PConvResNet --loss VGG19 --learning_rate 0.0005 --lr_size 32 --epochs 8 --ckpt_name pconvresnet_vgg
python inference.py --model PConvResNet --loss VGG19 --lr_size 32 --ckpt_name pconvresnet_vgg

python train.py --model PConvResNet --loss VGG19 --learning_rate 0.0005 --lr_size 32 --epochs 8 --ckpt_every 1 --ckpt_name pconvresnet_vgg
python inference.py --model PConvResNet --lr_size 32 --loss VGG19 --ckpt_name pconvresnet_vgg

